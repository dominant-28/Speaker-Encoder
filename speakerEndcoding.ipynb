{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from speakerendcoder import SpeakerEncoder\n",
    "import torch.nn.functional \n",
    "import torch.nn as nn  \n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from eval import evaluate_embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mel_spectrogram_transform(sample_rate, n_fft=1024, hop_length=256, n_mels=64):\n",
    "    return torchaudio.transforms.MelSpectrogram(\n",
    "        sample_rate=sample_rate,\n",
    "        n_fft=n_fft,\n",
    "        hop_length=hop_length,\n",
    "        n_mels=n_mels\n",
    "    )\n",
    "\n",
    "\n",
    "def prepare_dataset(data_dir, train_ratio=0.7):\n",
    "   \n",
    "    speaker_to_id = {}\n",
    "    id_to_speaker = {}\n",
    "    speaker_data = {} \n",
    "\n",
    "\n",
    "    max_speakers = 60\n",
    "\n",
    "    for speaker_id in sorted(os.listdir(os.path.join(data_dir, \"train-clean-100\")))[:max_speakers]:\n",
    "        speaker_path = os.path.join(data_dir, \"train-clean-100\", speaker_id)\n",
    "        if not os.path.isdir(speaker_path) or not speaker_id.isdigit():\n",
    "            continue\n",
    "        speaker_id = int(speaker_id)\n",
    "\n",
    "        if speaker_id not in speaker_to_id:\n",
    "            speaker_to_id[speaker_id] = len(speaker_to_id)\n",
    "            id_to_speaker[speaker_to_id[speaker_id]] = speaker_id\n",
    "\n",
    "        speaker_data[speaker_id] = []\n",
    "        \n",
    "        for chapter_id in os.listdir(speaker_path):\n",
    "            chapter_path = os.path.join(speaker_path, chapter_id)\n",
    "            if not os.path.isdir(chapter_path):\n",
    "                continue\n",
    "\n",
    "            text_dict = {}\n",
    "            for file in os.listdir(chapter_path):\n",
    "                if file.endswith(\".txt\"):\n",
    "                    with open(os.path.join(chapter_path, file), \"r\") as f:\n",
    "                        for line in f:\n",
    "                            line_ = line.strip().split()\n",
    "                            if len(line_) > 1:  \n",
    "                                file_prefix = line_[0]\n",
    "                                text_dict[file_prefix] = \" \".join(line_[1:])\n",
    "\n",
    "            for file in os.listdir(chapter_path):\n",
    "                if file.endswith(\".flac\"):\n",
    "                    file_prefix = file.replace(\".flac\", \"\")\n",
    "                    text_line = text_dict.get(file_prefix, \"\")\n",
    "                    audio_path = os.path.join(chapter_path, file)\n",
    "                    \n",
    "            \n",
    "                    try:\n",
    "                        info = torchaudio.info(audio_path)\n",
    "                        if info.num_frames / info.sample_rate >= 1.0: \n",
    "                            speaker_data[speaker_id].append((audio_path, speaker_to_id[speaker_id], text_line))\n",
    "                    except:\n",
    "                        continue\n",
    "\n",
    "\n",
    "    train_set = []\n",
    "    test_set = []\n",
    "    \n",
    "    print(f\"Found {len(speaker_data)} speakers\")\n",
    "    \n",
    "    for speaker_id, data in speaker_data.items():\n",
    "        if len(data) < 10: \n",
    "            continue\n",
    "            \n",
    "        random.shuffle(data)\n",
    "        split_point = int(len(data) * train_ratio)\n",
    "        train_set.extend(data[:split_point])\n",
    "        test_set.extend(data[split_point:])\n",
    "\n",
    "    random.shuffle(train_set)\n",
    "    random.shuffle(test_set)\n",
    "\n",
    "    print(f\"Total Train Samples: {len(train_set)}, Test Samples: {len(test_set)}\")\n",
    "    print(f\"Number of unique speakers: {len(speaker_to_id)}\")\n",
    "    \n",
    "    return train_set, test_set, speaker_to_id, id_to_speaker\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio.transforms as T\n",
    "import random\n",
    "import torch\n",
    "\n",
    "class LibriSpeechDataset(Dataset):\n",
    "    def __init__(self, data, transformation, target_sample_rate, num_mel_bins, device=None, max_time=400):\n",
    "        self.data = data\n",
    "        self.transformation = transformation\n",
    "        self.target_sample_rate = target_sample_rate\n",
    "        self.device = device if device else torch.device(\"cpu\")\n",
    "        self.max_time = max_time\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        audio_path, label, text = self.data[index]\n",
    "\n",
    "        try:\n",
    "            signal, sample_rate = torchaudio.load(audio_path)\n",
    "            \n",
    "            # Convert stereo to mono\n",
    "            if signal.shape[0] > 1:\n",
    "                signal = torch.mean(signal, dim=0, keepdim=True)\n",
    "            \n",
    "            # Resample to target sample rate\n",
    "            if sample_rate != self.target_sample_rate:\n",
    "                resampler = T.Resample(orig_freq=sample_rate, new_freq=self.target_sample_rate)\n",
    "                signal = resampler(signal)\n",
    "\n",
    "            \n",
    "            signal = self.apply_augmentations(signal, self.target_sample_rate)\n",
    "            \n",
    "            signal = signal.to(self.device)\n",
    "            mel_spec = self.transformation(signal)\n",
    "            \n",
    "            # Trim or pad to fixed size\n",
    "            if mel_spec.shape[2] > self.max_time:\n",
    "                start = random.randint(0, mel_spec.shape[2] - self.max_time)\n",
    "                mel_spec = mel_spec[:, :, start:start + self.max_time]\n",
    "            else:\n",
    "                padding = (0, max(0, self.max_time - mel_spec.shape[2]))\n",
    "                mel_spec = torch.nn.functional.pad(mel_spec, padding)\n",
    "\n",
    "            return mel_spec, label, text\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {audio_path}: {str(e)}\")\n",
    "            default_spec = torch.zeros(1, self.transformation.n_mels, self.max_time)\n",
    "            return default_spec, label, text\n",
    "\n",
    "    def apply_augmentations(self, waveform, sample_rate):\n",
    "        \"\"\"\n",
    "        Apply data augmentations to improve model robustness.\n",
    "        \"\"\"\n",
    "        augmentations = [\n",
    "            self.add_gaussian_noise\n",
    "        ]\n",
    "        random.shuffle(augmentations)  # Apply augmentations in random order\n",
    "\n",
    "        for aug in augmentations:\n",
    "            if random.random() < 0.4:  # 40% chance to apply each augmentation\n",
    "                waveform = aug(waveform, sample_rate)\n",
    "\n",
    "        return waveform\n",
    "\n",
    "    def random_time_masking(self, waveform, sample_rate):\n",
    "        \"\"\"Applies time masking to the waveform.\"\"\"\n",
    "        time_mask = T.TimeMasking(time_mask_param=80)  # Mask up to 80 time steps\n",
    "        return time_mask(waveform)\n",
    "\n",
    "    def random_frequency_masking(self, waveform, sample_rate):\n",
    "        \"\"\"Applies frequency masking to the waveform.\"\"\"\n",
    "        freq_mask = T.FrequencyMasking(freq_mask_param=30)  # Mask up to 30 frequency bins\n",
    "        return freq_mask(waveform)\n",
    "\n",
    "    def random_pitch_shift(self, waveform, sample_rate):\n",
    "        \"\"\"Shifts the pitch of the waveform randomly.\"\"\"\n",
    "        semitones = random.uniform(-2, 2)  # Shift pitch between -2 and +2 semitones\n",
    "        pitch_shift = T.PitchShift(sample_rate, n_steps=semitones)\n",
    "        return pitch_shift(waveform)\n",
    "\n",
    "    \n",
    "\n",
    "    def add_gaussian_noise(self, waveform, sample_rate):\n",
    "        \"\"\"Adds random Gaussian noise to the waveform.\"\"\"\n",
    "        noise = torch.randn_like(waveform) * 0.02  # Noise with standard deviation 0.02\n",
    "        return waveform + noise\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader(data, batch_size, sample_rate, num_mel_bins, device, shuffle=True):\n",
    "    mel_spectrogram_transform = get_mel_spectrogram_transform(sample_rate, n_mels=num_mel_bins)\n",
    "\n",
    "    dataset = LibriSpeechDataset(\n",
    "        data=data,\n",
    "        transformation=mel_spectrogram_transform.to(device),\n",
    "        target_sample_rate=sample_rate,\n",
    "        num_mel_bins=num_mel_bins,\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    dataloader = DataLoader(\n",
    "        dataset, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=shuffle, \n",
    "        drop_last=False\n",
    "    )\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, device, num_epochs=10):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2)\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        progress_bar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}')\n",
    "        for specs, labels, _ in progress_bar:\n",
    "            specs, labels = specs.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            embeddings, logits = model(specs)\n",
    "            loss = criterion(logits, labels)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            _, predicted = torch.max(logits.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            progress_bar.set_postfix({'loss': running_loss/len(train_loader), 'acc': 100. * correct / total})\n",
    "        \n",
    "        train_loss = running_loss / len(train_loader)\n",
    "        train_losses.append(train_loss)\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for specs, labels, _ in val_loader:\n",
    "                specs, labels = specs.to(device), labels.to(device)\n",
    "                \n",
    "                embeddings, logits = model(specs)\n",
    "                loss = criterion(logits, labels)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                \n",
    "                _, predicted = torch.max(logits.data, 1)\n",
    "                val_total += labels.size(0)\n",
    "                val_correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        val_loss = val_loss / len(val_loader)\n",
    "        val_losses.append(val_loss)\n",
    "        val_acc = 100. * val_correct / val_total\n",
    "        \n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%')\n",
    "        \n",
    "        # Learning rate scheduler\n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        # Save best model\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), 'best_speaker_encoder.pth')\n",
    "            print(\"Saved best model!\")\n",
    "    \n",
    "    # Plot training curve\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(train_losses, label='Training Loss')\n",
    "    plt.plot(val_losses, label='Validation Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.savefig('training_curve.png')\n",
    "    plt.close()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Set parameters\n",
    "    data_dir = r\"DIR_OF PROJECT\LibriSpeech\"  # Update with your path\n",
    "    sample_rate = 16000\n",
    "    num_mel_bins = 64\n",
    "    batch_size = 32\n",
    "    num_epochs = 70\n",
    "    embedding_dim = 256\n",
    "    \n",
    "    # Set device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Prepare dataset\n",
    "    train_set, test_set, speaker_to_id, id_to_speaker = prepare_dataset(data_dir)\n",
    "    num_speakers = len(speaker_to_id)\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader = create_dataloader(train_set, batch_size, sample_rate, num_mel_bins, device)\n",
    "    test_loader = create_dataloader(test_set, batch_size, sample_rate, num_mel_bins, device, shuffle=False)\n",
    "    \n",
    "    # Initialize model\n",
    "    model = SpeakerEncoder(num_speakers=num_speakers, embedding_dim=embedding_dim).to(device)\n",
    "    \n",
    "    # Train model\n",
    "    model = train_model(model, train_loader, test_loader, device, num_epochs=num_epochs)\n",
    "    \n",
    "    # Load best model\n",
    "    model.load_state_dict(torch.load('best_speaker_encoder.pth'))\n",
    "    \n",
    "    # Evaluate embeddings\n",
    "    evaluate_embeddings(model, test_loader, device, id_to_speaker)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "speaker_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
